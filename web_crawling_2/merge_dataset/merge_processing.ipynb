{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NT550-045\\Desktop\\ml2\\web_crawling\n"
     ]
    }
   ],
   "source": [
    "gp_path = f\"{os.getcwd().split('web_crawling')[0]}web_crawling\"\n",
    "print(gp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "steampy_path = glob(f'{gp_path}\\\\**\\\\steampy\\\\appid_*.csv', recursive=True)[::2]\n",
    "user_peak = glob(f'{gp_path}\\\\**\\\\MPW\\\\**\\\\*_user.csv', recursive=True)[:5]\n",
    "review_path = glob(f'{gp_path}\\\\**\\\\MPW\\\\**\\\\*_review.csv', recursive=True)[:5]\n",
    "steam_store_path = glob(f'{gp_path}\\\\**\\\\CGW\\\\**\\\\final\\\\*.csv', recursive=True)\n",
    "store_date_path = glob(f'{gp_path}\\\\**\\\\CGW\\\\**\\\\csvs\\\\*_date.csv', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\LSH\\\\steampy\\\\appid_2018.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\LSH\\\\steampy\\\\appid_2019.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\LSH\\\\steampy\\\\appid_2020.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\LSH\\\\steampy\\\\appid_2021.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\LSH\\\\steampy\\\\appid_2022.csv']\n",
      "\n",
      "['c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\CGW\\\\data_handling\\\\final\\\\steam_store_after_pcs_2018.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\CGW\\\\data_handling\\\\final\\\\steam_store_after_pcs_2019.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\CGW\\\\data_handling\\\\final\\\\steam_store_after_pcs_2020.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\CGW\\\\data_handling\\\\final\\\\steam_store_after_pcs_2021.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\CGW\\\\data_handling\\\\final\\\\steam_store_after_pcs_2022.csv']\n",
      "\n",
      "['c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\MPW\\\\DATA\\\\USER_PEAK\\\\2018_user.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\MPW\\\\DATA\\\\USER_PEAK\\\\2019_user.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\MPW\\\\DATA\\\\USER_PEAK\\\\2020_user.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\MPW\\\\DATA\\\\USER_PEAK\\\\2021_user.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\MPW\\\\DATA\\\\USER_PEAK\\\\2022_user.csv']\n",
      "\n",
      "['c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\MPW\\\\DATA\\\\REVIEW\\\\2018_review.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\MPW\\\\DATA\\\\REVIEW\\\\2019_review.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\MPW\\\\DATA\\\\REVIEW\\\\2020_review.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\MPW\\\\DATA\\\\REVIEW\\\\2021_review.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\MPW\\\\DATA\\\\REVIEW\\\\2022_review.csv']\n",
      "\n",
      "['c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\CGW\\\\data_handling\\\\csvs\\\\2018_date.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\CGW\\\\data_handling\\\\csvs\\\\2019_date.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\CGW\\\\data_handling\\\\csvs\\\\2020_date.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\CGW\\\\data_handling\\\\csvs\\\\2021_date.csv', 'c:\\\\Users\\\\NT550-045\\\\Desktop\\\\ml2\\\\web_crawling\\\\CGW\\\\data_handling\\\\csvs\\\\2022_date.csv']\n"
     ]
    }
   ],
   "source": [
    "print(steampy_path,steam_store_path,user_peak,review_path, store_date_path, sep='\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# 스팀스파이 파일\n",
    "t_spy = pd.read_csv(steampy_path[0], sep='\\t')\n",
    "\n",
    "# 스팀차트 파일\n",
    "# 불필요한 컬럼 제거 ; Unamed 0 : 0 / usecols 옵션사용\n",
    "t_peak = pd.read_csv(user_peak[0], sep=';', usecols=['App_id', '24_Hour_Peak', 'All_time_peak'])\n",
    "# App_id 컬럼 이름 변경\n",
    "t_peak.rename(columns={'App_id':'appid'},inplace=True)\n",
    "\n",
    "# 스팀 타켓값 파일\n",
    "# 불필요한 컬럼 제거 ; Unamed 0 : 0 / usecols 옵션사용\n",
    "t_review = pd.read_csv(review_path[0], sep=';', usecols=['App_id', 'Review'])\n",
    "# App_id 컬럼 이름 변경\n",
    "t_review.rename(columns={'App_id':'appid'},inplace=True)\n",
    "\n",
    "# 스팀 스토어 파일\n",
    "t_store = pd.read_csv(steam_store_path[0], sep=';')\n",
    "# 불필요한 컬럼 제거 ; Unamed 0 : 0 / 컬럼 인덱싱\n",
    "t_store = t_store[t_store.columns[1:]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Overwhelmingly Positive' 'Very Positive' 'Positive' 'Mostly Positive'\n",
      " 'Mixed' 'No user reviews' '8 user reviews' '7 user reviews'\n",
      " '4 user reviews' '9 user reviews' '5 user reviews' '6 user reviews'\n",
      " '2 user reviews' '1 user reviews' '3 user reviews' 'Mostly Negative'\n",
      " 'Negative' 'Very Negative']\n",
      "18\n",
      "\n",
      "['Overwhelmingly Positive' 'Very Positive' 'Mostly Positive' 'Mixed'\n",
      " 'Positive' '6 user reviews' 'No user reviews' '4 user reviews'\n",
      " '3 user reviews' '8 user reviews' '5 user reviews' '7 user reviews'\n",
      " '9 user reviews' '1 user reviews' '2 user reviews' 'Mostly Negative'\n",
      " 'Very Negative' 'Negative']\n",
      "18\n",
      "\n",
      "['Overwhelmingly Positive' 'Very Positive' 'Mixed' 'Mostly Positive'\n",
      " 'No user reviews' 'Positive' '2 user reviews' '4 user reviews'\n",
      " '8 user reviews' '6 user reviews' '7 user reviews' 'None'\n",
      " '9 user reviews' '1 user reviews' '5 user reviews' '3 user reviews'\n",
      " 'Mostly Negative' 'Negative' 'Very Negative']\n",
      "19\n",
      "\n",
      "['Overwhelmingly Positive' 'Very Positive' 'Mostly Positive' 'Mixed'\n",
      " 'Positive' 'None' '8 user reviews' '9 user reviews' 'No user reviews'\n",
      " '6 user reviews' '1 user reviews' '7 user reviews' '5 user reviews'\n",
      " '2 user reviews' '3 user reviews' '4 user reviews' 'Mostly Negative'\n",
      " 'Very Negative' 'Negative' 'Overwhelmingly Negative']\n",
      "20\n",
      "\n",
      "['Overwhelmingly Positive' 'Very Positive' 'Mostly Positive' 'Positive'\n",
      " 'Mixed' 'Mostly Negative' '6 user reviews' 'No user reviews'\n",
      " '8 user reviews' '7 user reviews' '1 user reviews' '3 user reviews'\n",
      " '9 user reviews' '4 user reviews' '5 user reviews' '2 user reviews'\n",
      " 'Very Negative' 'Negative' 'Overwhelmingly Negative']\n",
      "19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spy_del = ['developer', 'publisher', 'tags', 'languages']\n",
    "\n",
    "review_set = set()\n",
    "merge_df = pd.DataFrame()\n",
    "\n",
    "for i in range(5):\n",
    "    t_spy = pd.read_csv(steampy_path[i], sep='\\t')\n",
    "    t_spy.drop(spy_del, axis=1, inplace=True)\n",
    "    \n",
    "    t_peak = pd.read_csv(user_peak[i], sep=';', usecols=['App_id', '24_Hour_Peak', 'All_time_peak'])\n",
    "    t_peak.rename(columns={'App_id':'appid'},inplace=True)\n",
    "\n",
    "    t_review = pd.read_csv(review_path[i], sep=';', usecols=['App_id', 'Review'])\n",
    "    t_review.rename(columns={'App_id':'appid'},inplace=True)\n",
    "\n",
    "    t_store = pd.read_csv(steam_store_path[i], sep=';')\n",
    "    t_store = t_store[t_store.columns[1:-2]]\n",
    "    \n",
    "    t_date = pd.read_csv(store_date_path[i])\n",
    "    \n",
    "    t_merge = t_peak.merge(t_spy).merge(t_store).merge(t_date).merge(t_review)\n",
    "    \n",
    "    # 각 파일마다 타켓의 유니크한 값 확인\n",
    "    print(t_merge['Review'].unique())\n",
    "    print(len(t_merge['Review'].unique()))\n",
    "    print()\n",
    "    \n",
    "    # 유니크한 타켓 저장\n",
    "    review_set |= set(t_merge['Review'].unique())\n",
    "    \n",
    "    # 각 연도 파일 합치기\n",
    "    merge_df = pd.concat([merge_df, t_merge])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tag 파일 정리\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\reot1\\\\OneDrive\\\\바탕 화면\\\\ml2\\\\web_crawling\\\\LSH\\\\steampy\\\\appid_2018_tag.csv',\n",
       " 'c:\\\\Users\\\\reot1\\\\OneDrive\\\\바탕 화면\\\\ml2\\\\web_crawling\\\\LSH\\\\steampy\\\\appid_2019_tag.csv',\n",
       " 'c:\\\\Users\\\\reot1\\\\OneDrive\\\\바탕 화면\\\\ml2\\\\web_crawling\\\\LSH\\\\steampy\\\\appid_2020_tag.csv',\n",
       " 'c:\\\\Users\\\\reot1\\\\OneDrive\\\\바탕 화면\\\\ml2\\\\web_crawling\\\\LSH\\\\steampy\\\\appid_2021_tag.csv',\n",
       " 'c:\\\\Users\\\\reot1\\\\OneDrive\\\\바탕 화면\\\\ml2\\\\web_crawling\\\\LSH\\\\steampy\\\\appid_2022_tag.csv']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_path = glob(f'{gp_path}\\\\**\\\\steampy\\\\*tag.csv', recursive=True)\n",
    "tag_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7713\n",
      "7370\n",
      "9063\n",
      "10715\n",
      "11862\n"
     ]
    }
   ],
   "source": [
    "tag_merge = pd.DataFrame()\n",
    "for path in tag_path:\n",
    "    t_tag = pd.read_csv(path)\n",
    "    \n",
    "    # 태그 파일 합치기\n",
    "    tag_merge = pd.concat([tag_merge, t_tag])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "데이터 유실 확인\n",
    "\n",
    "게임 출시일자 추출시 해당 사이트에서 기존에 있던 게임 중 \n",
    "삭제되거나 문제있는 게임을 목록에서 제거한 것으로 보임.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7635\n",
      "7713 7713 7713 7713 7639\n",
      "\n",
      "7306\n",
      "7370 7370 7370 7370 7313\n",
      "\n",
      "8979\n",
      "9063 9063 9063 9063 8985\n",
      "\n",
      "10600\n",
      "10715 10715 10715 10715 10606\n",
      "\n",
      "11732\n",
      "11862 11862 11862 11862 11745\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    t_spy = pd.read_csv(steampy_path[i], sep='\\t')\n",
    "    t_spy.drop(spy_del, axis=1, inplace=True)\n",
    "    \n",
    "    t_peak = pd.read_csv(user_peak[i], sep=';', usecols=['App_id', '24_Hour_Peak', 'All_time_peak'])\n",
    "    t_peak.rename(columns={'App_id':'appid'},inplace=True)\n",
    "\n",
    "    t_review = pd.read_csv(review_path[i], sep=';', usecols=['App_id', 'Review'])\n",
    "    t_review.rename(columns={'App_id':'appid'},inplace=True)\n",
    "\n",
    "    t_store = pd.read_csv(steam_store_path[i], sep=';')\n",
    "    t_store = t_store[t_store.columns[1:-2]]\n",
    "    \n",
    "    t_date = pd.read_csv(store_date_path[i])\n",
    "    \n",
    "    t_merge = t_peak.merge(t_spy).merge(t_store).merge(t_date).merge(t_review)\n",
    "    print(len(t_merge))\n",
    "    print(len(t_spy), len(t_peak), len(t_review), len(t_store), len(t_date))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Very Positive              7498\n",
       "Positive                   6950\n",
       "Mixed                      6575\n",
       "Mostly Positive            4701\n",
       "1 user reviews             3706\n",
       "2 user reviews             2908\n",
       "3 user reviews             2343\n",
       "4 user reviews             1940\n",
       "No user reviews            1810\n",
       "5 user reviews             1669\n",
       "6 user reviews             1423\n",
       "7 user reviews             1244\n",
       "8 user reviews             1100\n",
       "Mostly Negative             957\n",
       "9 user reviews              948\n",
       "Overwhelmingly Positive     270\n",
       "Negative                    185\n",
       "Very Negative                17\n",
       "None                          5\n",
       "Overwhelmingly Negative       3\n",
       "Name: Review, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df['Review'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive        0.419852\n",
       "Indifference    0.412869\n",
       "Mixed           0.142156\n",
       "Negative        0.025123\n",
       "Name: Review, dtype: float64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df['Review'].apply(lambda x:'Indifference' if 'user' in x  or 'None' == x else x).apply(lambda x: 'Negative' if 'Negative' in x else x).apply(lambda x: 'Positive' if 'Positive' in x else x ).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\reot1\\\\OneDrive\\\\바탕 화면\\\\ml2\\\\web_crawling\\\\merge_dataset'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = glob(f'{gp_path}\\\\**\\\\merge_dataset', recursive=True)[0]\n",
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.to_csv(f'{dataset_path}\\\\raw_merge.csv', sep='\\t', index=False)\n",
    "tag_merge.to_csv(f'{dataset_path}\\\\tag_merge.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
